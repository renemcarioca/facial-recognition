{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from preprocess import Preprocess\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x1, x2, label):\n",
    "        dist = nn.functional.pairwise_distance(x1, x2)\n",
    "        # label 1 means similar, 0 means dissimilar\n",
    "        # when similar, loss is the distance\n",
    "        # when dissimilar and more distant than the margin, no loss\n",
    "        # when dissimilar and closer than the margin, loss is the distance to the margin\n",
    "        loss = label * torch.pow(dist, 2) + (1 - label) * torch.pow(torch.clamp(self.margin - dist, min=0.0), 2)\n",
    "        loss = torch.mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_UNFROZEN_BLOCKS = 3\n",
    "\n",
    "class TimmSiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimmSiameseNetwork, self).__init__()\n",
    "        # https://huggingface.co/timm/vit_mediumd_patch16_reg4_gap_256.sbb_in12k_ft_in1k\n",
    "        self.model = timm.create_model(\n",
    "            'vit_mediumd_patch16_reg4_gap_256.sbb_in12k_ft_in1k',\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # remove classifier nn.Linear\n",
    "        )\n",
    "        # Freeze all layers except the last conv\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for block in self.model.blocks[-NUM_OF_UNFROZEN_BLOCKS:]:\n",
    "            for param in block.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # get model specific transforms (normalization, resize)\n",
    "        data_config = timm.data.resolve_model_data_config(self.model)\n",
    "        self.transforms = timm.data.create_transform(**data_config, is_training=True)\n",
    "\n",
    "    def forward_once(self, img) -> torch.Tensor:\n",
    "        return self.model(self.transforms(img))\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        return self.forward_once(img1), self.forward_once(img2)\n",
    "\n",
    "timm_model = TimmSiameseNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "train_dataset = Preprocess.load_train_pairs(transform=TRANSFORM)\n",
    "val_dataset = Preprocess.load_test_pairs(transform=TRANSFORM)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(timm_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "def train_loop(model):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (img1, img2, labels) in enumerate(train_loader):\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (output1 - output2).pow(2).sum(dim=1).sqrt().lt(0.5).eq(labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            print(f\"Batch [{batch_idx+1}/{len(train_loader)}], Train Loss: {loss.item():.4f}, Train Acc: {train_correct/train_total:.4f}        \", end='\\r')\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for img1, img2, labels in val_loader:\n",
    "                img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "                output1, output2 = model(img1, img2)\n",
    "                loss = criterion(output1, output2, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (output1 - output2).pow(2).sum(dim=1).sqrt().lt(0.5).eq(labels).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_train_loss:.4f}, Train Acc: {train_correct/train_total:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_siamese_model.pth')\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "train_loop(timm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Preprocess.load_train_pairs()\n",
    "val_data = Preprocess.load_test_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimmSiameseNetwork()\n",
    "model.load_state_dict(torch.load('trained/best_siamese_model_33epochs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(data, model, threshold=0.5):\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred = []\n",
    "        y_true = []\n",
    "        for i, (img1, img2, label) in enumerate(data):\n",
    "            img1 = img1.to(device)\n",
    "            img2 = img2.to(device)\n",
    "            output1, output2 = model(img1.unsqueeze(0), img2.unsqueeze(0))\n",
    "            euclidean_distance = nn.PairwiseDistance()(output1.squeeze(), output2.squeeze())\n",
    "            prediction = int(euclidean_distance < threshold)\n",
    "            pred.append(prediction)\n",
    "            y_true.append(label)\n",
    "            print(f'Progress: {i}/{len(data)}        ', end='\\r')\n",
    "        return pred, y_true\n",
    "\n",
    "pred, y_true = predictions(val_data, model, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_true, pred)}')\n",
    "print(f'Recall: {recall_score(y_true, pred)}')\n",
    "print(f'Precision: {precision_score(y_true, pred)}')\n",
    "print(f'F1: {f1_score(y_true, pred)}')\n",
    "print(f'ROC AUC: {roc_auc_score(y_true, pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
